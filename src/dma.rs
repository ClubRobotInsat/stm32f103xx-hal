#![allow(dead_code)]

use core::marker::PhantomData;
use core::ops;

use rcc::AHB;

pub trait Static<B> {
    fn borrow(&self) -> &B;
}

impl<B> Static<B> for &'static B {
    fn borrow(&self) -> &B {
        *self
    }
}

impl<B> Static<B> for &'static mut B {
    fn borrow(&self) -> &B {
        *self
    }
}

pub trait DmaExt {
    type Channels;

    fn split(self, ahb: &mut AHB) -> Self::Channels;
}

pub struct Transfer<MODE, BUFFER, CHANNEL, PAYLOAD> {
    _mode: PhantomData<MODE>,
    buffer: BUFFER,
    channel: CHANNEL,
    payload: PAYLOAD,
}

impl<BUFFER, CHANNEL, PAYLOAD> Transfer<R, BUFFER, CHANNEL, PAYLOAD> {
    pub(crate) fn r(buffer: BUFFER, channel: CHANNEL, payload: PAYLOAD) -> Self {
        Transfer {
            _mode: PhantomData,
            buffer,
            channel,
            payload,
        }
    }
}

impl<BUFFER, CHANNEL, PAYLOAD> Transfer<W, BUFFER, CHANNEL, PAYLOAD> {
    pub(crate) fn w(buffer: BUFFER, channel: CHANNEL, payload: PAYLOAD) -> Self {
        Transfer {
            _mode: PhantomData,
            buffer,
            channel,
            payload,
        }
    }
}

impl<BUFFER, CHANNEL, PAYLOAD> ops::Deref for Transfer<R, BUFFER, CHANNEL, PAYLOAD> {
    type Target = BUFFER;

    fn deref(&self) -> &BUFFER {
        &self.buffer
    }
}

/// Read transfer
pub struct R;

/// Write transfer
pub struct W;

macro_rules! dma {
    ($($DMAX:ident: ($dmaX:ident, $dmaXen:ident, $dmaXrst:ident, {
        $($CX:ident: (
            $ccrX:ident,
            $CCRX:ident,
            $cndtrX:ident,
            $CNDTRX:ident,
            $cparX:ident,
            $CPARX:ident,
            $cmarX:ident,
            $CMARX:ident,
            $tcifX:ident,
            $cgifX:ident
        ),)+
    }),)+) => {
        $(
            pub mod $dmaX {
                use core::sync::atomic::{self, Ordering};

                use stm32f103xx::{$DMAX, dma1};

                use dma::{DmaExt, Transfer};
                use rcc::AHB;

                pub struct Channels((), $(pub $CX),+);

                $(
                    pub struct $CX { _0: () }

                    impl $CX {
                        pub(crate) fn isr(&self) -> dma1::isr::R {
                            // NOTE(unsafe) atomic read with no side effects
                            unsafe { (*$DMAX::ptr()).isr.read() }
                        }

                        pub(crate) fn ifcr(&self) -> &dma1::IFCR {
                            unsafe { &(*$DMAX::ptr()).ifcr }
                        }

                        pub(crate) fn ccr(&mut self) -> &dma1::$CCRX {
                            unsafe { &(*$DMAX::ptr()).$ccrX }
                        }

                        pub(crate) fn cndtr(&mut self) -> &dma1::$CNDTRX {
                            unsafe { &(*$DMAX::ptr()).$cndtrX }
                        }

                        pub(crate) fn cpar(&mut self) -> &dma1::$CPARX {
                            unsafe { &(*$DMAX::ptr()).$cparX }
                        }

                        pub(crate) fn cmar(&mut self) -> &dma1::$CMARX {
                            unsafe { &(*$DMAX::ptr()).$cmarX }
                        }
                    }

                    impl<BUFFER, PAYLOAD, MODE> Transfer<MODE, BUFFER, $CX, PAYLOAD> {
                        pub fn wait(self) -> (BUFFER, $CX, PAYLOAD) {
                            // XXX should we check for transfer errors here?
                            // The manual says "A DMA transfer error can be generated by reading
                            // from or writing to a reserved address space". I think it's impossible
                            // to get to that state with our type safe API and *safe* Rust.
                            while self.channel.isr().$tcifX().bit_is_clear() {}

                            self.channel.ifcr().write(|w| w.$cgifX().set_bit());

                            // TODO can we weaken this compiler barrier?
                            // NOTE(compiler_fence) operations on `buffer` should not be reordered
                            // before the previous statement, which marks the DMA transfer as done
                            atomic::compiler_fence(Ordering::SeqCst);

                            (self.buffer, self.channel, self.payload)
                        }
                    }

                )+

                impl DmaExt for $DMAX {
                    type Channels = Channels;

                    fn split(self, ahb: &mut AHB) -> Channels {
                        ahb.enr().modify(|_, w| w.$dmaXen().enabled());

                        // reset the DMA control registers (stops all on-going transfers)
                        $(
                            self.$ccrX.reset();
                        )+

                        Channels((), $($CX { _0: () }),+)
                    }
                }
            }
        )+
    }
}

dma! {
    DMA1: (dma1, dma1en, dma1rst, {
        C1: (ccr1, CCR1, cndtr1, CNDTR1, cpar1, CPAR1, cmar1, CMAR1, tcif1, cgif1),
        C2: (ccr2, CCR2, cndtr2, CNDTR2, cpar2, CPAR2, cmar2, CMAR2, tcif2, cgif2),
        C3: (ccr3, CCR3, cndtr3, CNDTR3, cpar3, CPAR3, cmar3, CMAR3, tcif3, cgif3),
        C4: (ccr4, CCR4, cndtr4, CNDTR4, cpar4, CPAR4, cmar4, CMAR4, tcif4, cgif4),
        C5: (ccr5, CCR5, cndtr5, CNDTR5, cpar5, CPAR5, cmar5, CMAR5, tcif5, cgif5),
        C6: (ccr6, CCR6, cndtr6, CNDTR6, cpar6, CPAR6, cmar6, CMAR6, tcif6, cgif6),
        C7: (ccr7, CCR7, cndtr7, CNDTR7, cpar7, CPAR7, cmar7, CMAR7, tcif7, cgif7),
    }),

    DMA2: (dma2, dma2en, dma2rst, {
        C1: (ccr1, CCR1, cndtr1, CNDTR1, cpar1, CPAR1, cmar1, CMAR1, tcif1, cgif1),
        C2: (ccr2, CCR2, cndtr2, CNDTR2, cpar2, CPAR2, cmar2, CMAR2, tcif2, cgif2),
        C3: (ccr3, CCR3, cndtr3, CNDTR3, cpar3, CPAR3, cmar3, CMAR3, tcif3, cgif3),
        C4: (ccr4, CCR4, cndtr4, CNDTR4, cpar4, CPAR4, cmar4, CMAR4, tcif4, cgif4),
        C5: (ccr5, CCR5, cndtr5, CNDTR5, cpar5, CPAR5, cmar5, CMAR5, tcif5, cgif5),
    }),
}
